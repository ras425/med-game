{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "N-8mxoG5nIkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSYNtSIMWzRa",
        "outputId": "309c6fda-43d7-4184-b2cb-4ac596000738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.0-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0eC9gsJXpH1",
        "outputId": "b41d9e75-cc23-49b8-8dac-9600397b130a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaGXqK9BWuQT"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import ast  # for converting embeddings saved as strings back to arrays\n",
        "import openai  # for calling the OpenAI API\n",
        "import pandas as pd  # for storing text and embeddings data\n",
        "import tiktoken  # for counting tokens\n",
        "from scipy import spatial  # for calculating vector similarities for search\n",
        "\n",
        "\n",
        "# models\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "GPT_MODEL = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZEe4JKvkwa6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "ytEZx5iXYBXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test using copy-pasted text, limit on length"
      ],
      "metadata": {
        "id": "9kuEugcXLOMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "case_study = \"\"\"\n",
        "\n",
        " CLINICAL HISTORY\n",
        "\n",
        "A man in his 80's who was admitted to our hospital and was being treated for pneumonia,\n",
        " complained of a slow growing, non-healing lesion of unknown duration near his right shoulder.\n",
        " Patient's medical history was significant for atrial fibrillation, and COPD and surgical history was significant\n",
        " for a Basal cell carcinoma on nasal tip. Clinical examination revealed 2.0 cm ulcerated plaque with pink border on\n",
        " the right shoulder (Figure 1) and another pink plaque with crust on the left frontal scalp which measured 1.4 cm in\n",
        " maximum dimension. Considering the patient's past history and clinical presentation of the lesions; basal cell carcinoma\n",
        " was high on differential and both of these lesions were treated with excisional biopsy with electrodessication and curettage\n",
        " and the specimen was sent for histopathological examination.\n",
        "\n",
        " On histopathological examination the lesion on the scalp showed features consistent with superficial and nodular types\n",
        "  of basal cell carcinoma while the lesion on the right shoulder interestingly showed epidermal ulceration and\n",
        "  proliferation of infiltrating glandular structures within the dermis (Figure 2). The glands were composed of\n",
        "  markedly pleomorphic squamoid cells (Figure 3) and contained intraluminal secretions. There was significant\n",
        "  atypia with presence of mitosis, tumor cell necrosis and foci of perineural invasion (Figures 4A, 4B). On\n",
        "  immunohistochemistry the neoplastic cells were positive for p63 (Figure 5), and weak positive for GATA3. CEA\n",
        "   showed intraluminal staining pattern (Figure 6). Mucicarmine stain was negative for presence of intracytoplasmic mucin.\n",
        "\n",
        "DIAGNOSIS\n",
        "\n",
        "Squamoid eccrine ductal carcinoma\n",
        "\n",
        "DISCUSSION\n",
        "\n",
        "Eccrine carcinomas comprise around 0.01% of all tumors and encompasses malignancies which originate from the eccrine glands(1). Squamoid eccrine ductal carcinoma (SEDC) was first described by Wong et al in 1997 for lesions which show eccrine ductal differentiation along with a prominent squamous component (2, 3). It is a rare tumor and with only few cases reported in the English literature. The largest series discussing about the SEDC is from Van der Horst et al (4) in 2016. SEDC has been frequently misdiagnosed as the squamous cell carcinoma (SCC) and other eccrine tumors. We present an interesting case of an elderly man who was simultaneously diagnosed with basal cell carcinoma (BCC) and SEDC.\n",
        "\n",
        "SEDC are rare tumors and are usually seen in seventh to eighth decade with an age range of 30-91 years. The tumor shows a slight male predominance (3-5) and usually presents as a slow growing nodule or plaque with ulceration or crusting and most frequently involving the head and neck region(3, 5). Few cases have been reported to involve extremities and trunk (4). These tumors have been reported to be present for years sometime up to 10 years before being diagnosed (3, 6). The clinical presentation of these tumors is nonspecific and are clinically thought to be BCC or SCC (4). Our patient also complained of a long-standing, non-healing ulcerated lesion which was clinically thought to be BCC.\n",
        "\n",
        "Histologically the tumor exhibits a biphasic pattern of growth with the superficial component showing close resemblance to squamous cell carcinoma while the deeper component showing evidence of duct differentiation with features of eccrine ductal carcinoma (3, 4). These tumors shows marked cytologic atypia with desmoplastic stroma and cords of deeply infiltrative tumor cells extending into the dermis and subcutaneous tissue (7). Mitoses can be frequently seen along with perineural invasion. Rarely tumor necrosis and evidence of lymphovascular invasion may be present (8). Our case showed presence of frequent mitosis with few atypical mitosis and foci of tumor necrosis with evidence of perineural invasion.\n",
        "\n",
        "The differential diagnosis includes squamous cell carcinoma, porocarcinoma with squamous differentiation, microcytic adnexal carcinoma, and also cutaneous metastasis from visceral tumors(1). Adequate sampling and histologic examination are necessary to demonstrate the ductal differentiation in the deeper component to avoid misdiagnosing this tumor as squamous cell carcinoma. Chhibber et al, reported a case of SEDC which was misdiagnosed as SCC multiple times on biopsy sample (1, 5).\n",
        "\n",
        "Porocarcinoma usually affects the lower extremity while SEDC predominantly involves the head and neck region and shows presence of tumor cells having poroid characteristics, in a background of a pre-existing poroma (8). Microcystic adnexal carcinoma usually show presence of keratinous cystic structures and lacks significant cytologic atypia which are helpful in differentiating it from SEDC (4, 6). Positivity for p63 immunostain favors a primary cutaneous sweat gland carcinoma over a metastatic adenocarcinoma to the skin (9). Our case demonstrated positive staining for p63 along with negative mucicarmine favoring a primary cutaneous malignancy rather than metastatic disease. Luminal differentiation can be demonstrated using epithelial membrane antigen (EMA) and carcinoembryonic antigen (CEA) immunostains. As EMA and CEA are typically seen in glandular tissue positivity for these immunostains is supportive of an adnexal origin and are typically negative in epithelial malignancies such as SCC (10).\n",
        "\n",
        "SEDC is regarded to have a low-grade malignant potential with few cases showing involvement of regional lymph nodes. However, the risk for local recurrence is high due to underestimation of size on gross examination, deeply infiltrative nature of the tumor and perineural invasion (5). There incidence of local recurrence in this tumor has been reported close to 25% (4). Kim et al has reported local recurrence rate of 10 to 70 percent after conventional excision and 0 to 5 percent after Moh's micrographic surgery (MMS) at an average of 30.9 months follow up (8, 11). There is also significant risk for metastasis to lymph nodes with rates approaching 13% (4). MMS has been suggested as an effective and tissue-sparing surgical modality by few authors (5). The disease-related mortality has not yet been reported in the literature, and the true biologic potential of these tumors is still unknown due to lack of larger studies with long follow-up. We think the tumor needs wide surgical excision with wide negative margins along with close clinical follow-up.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pJvIX4WxY4lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what are the patient's symptoms?\""
      ],
      "metadata": {
        "id": "YkXPaD0DIZZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"Use the article provided to answer the questions the user asks about the patient. DO NOT reveal information that comes after the DIAGNOSIS header until the user asks, reveal diagnosis\n",
        "\n",
        "Article:\n",
        "\\\"\\\"\\\"\n",
        "{case_study}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    messages=[\n",
        "        {'role': 'system', 'content': 'You answer questions about the case study.'},\n",
        "        {'role': 'user', 'content': query},\n",
        "    ],\n",
        "    model=GPT_MODEL,\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLelERUSZBXl",
        "outputId": "6dbbd015-ce7b-45ea-d246-fb0d5f1ecc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The patient complained of a slow-growing, non-healing lesion of unknown duration near his right shoulder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test using an input csv file (pre-chunked and pre-computed embeddings)"
      ],
      "metadata": {
        "id": "7bpX4WpULCyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download pre-chunked text and pre-computed embeddings\n",
        "# this file is ~200 MB, so may take a minute depending on your connection speed\n",
        "embeddings_path = \"https://cdn.openai.com/API/examples/data/winter_olympics_2022.csv\"\n",
        "\n",
        "df = pd.read_csv(embeddings_path)"
      ],
      "metadata": {
        "id": "Yc5QiNgIggWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert embeddings from CSV str type back to list type\n",
        "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "IuSw9w8Wgi2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search function\n",
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 100\n",
        ") -> tuple[list[str], list[float]]:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "    query_embedding_response = openai.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=query,\n",
        "    )\n",
        "    query_embedding = query_embedding_response.data[0].embedding\n",
        "    strings_and_relatednesses = [\n",
        "        (row.text, relatedness_fn(query_embedding, row.embedding))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    return strings[:top_n], relatednesses[:top_n]"
      ],
      "metadata": {
        "id": "_Ui-eZr2go0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "def query_message(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    model: str,\n",
        "    token_budget: int\n",
        ") -> str:\n",
        "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
        "    introduction = 'Use the article provided to answer the questions the user asks about the 2022 winter olympics.'\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "    message = introduction\n",
        "    for string in strings:\n",
        "        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
        "        if (\n",
        "            num_tokens(message + next_article + question, model=model)\n",
        "            > token_budget\n",
        "        ):\n",
        "            break\n",
        "        else:\n",
        "            message += next_article\n",
        "    return message + question\n",
        "\n",
        "\n",
        "def ask(\n",
        "    query: str,\n",
        "    df: pd.DataFrame = df,\n",
        "    model: str = GPT_MODEL,\n",
        "    token_budget: int = 4096 - 500,\n",
        "    print_message: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
        "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
        "    if print_message:\n",
        "        print(message)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You answer questions about the 2022 Winter Olympics.\"},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    response_message = response.choices[0].message.content\n",
        "    return response_message"
      ],
      "metadata": {
        "id": "mh2wqFZug2bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask('tell me about the 2022 winter olympics')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "9euMhkg9g8A3",
        "outputId": "3d8e6736-a833-4846-fef6-89fee4317472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The 2022 Winter Olympics, officially known as the XXIV Olympic Winter Games, took place from 4 to 20 February 2022 in Beijing, China, and surrounding areas. It was the 24th edition of the Winter Olympic Games. The Games featured 109 events across 15 disciplines, with new events such as big air freestyle skiing and women's monobob making their Olympic debuts. A total of 2,871 athletes from 91 nations competed, with countries like Haiti and Saudi Arabia making their Winter Olympic debuts. The host nation, China, finished third in the medal table with nine gold medals, marking its most successful performance in Winter Olympics history. The Games were held amidst concerns and controversies, including issues related to human rights violations in China and the impact of the COVID-19 pandemic.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A possible framework for game"
      ],
      "metadata": {
        "id": "VdDXe0sjmZOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:\n",
        "\n",
        "#https://community.openai.com/t/how-to-prevent-chatgpt-from-answering-questions-that-are-outside-the-scope-of-the-provided-context-in-the-system-role-message/112027\n",
        "#idk why i have to include case study info in messages after model=chatbot_model\n",
        "\n",
        "#also i dont think i need to chunk/embeddings bc the case studies are short enough"
      ],
      "metadata": {
        "id": "7xuf1nD5xufz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Pre-train on a given set of case studies\n",
        "def pretrain(case_studies):\n",
        "    trained_models = []\n",
        "    for case_study in case_studies:\n",
        "        messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You answer questions about the case study without revealing the diagnosis.\"},\n",
        "        {\"role\": \"user\", \"content\": case_study['description'] + case_study['details'] + case_study['diagnosis']},\n",
        "    ]\n",
        "        response = openai.chat.completions.create(\n",
        "            model=GPT_MODEL,\n",
        "            messages=messages,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        trained_models.append((response.model, case_study['description'], case_study['details'], case_study['diagnosis']))\n",
        "    return trained_models\n",
        "\n",
        "# Select a pre-trained model for a given round\n",
        "def select_model(trained_models):\n",
        "    return random.sample(trained_models, 1)\n",
        "\n",
        "# Play a round of the game\n",
        "def play_round(chatbot_model):\n",
        "    model, description, details, diagnosis = chatbot_model[0]\n",
        "\n",
        "    print(\"New round started.\")\n",
        "    print(\"Here's a brief description of the patient's case:\")\n",
        "    print(description)\n",
        "\n",
        "    #TODO:\n",
        "    #type 'guess' to guess diagnosis. should have responses like \"close\" or \"be more specific\" maybe\n",
        "    print(\"Ask about specific details to diagnose the patient. Type 'reveal' to reveal the diagnosis. Type 'quit' to end the round.\")\n",
        "    user_input = input(\"Your question or command: \")\n",
        "\n",
        "    while user_input.lower() != 'quit':\n",
        "        if user_input.lower() == 'reveal':\n",
        "            print(f\"The diagnosis is: {diagnosis}\")\n",
        "        else:\n",
        "            # Respond to the user's question\n",
        "            #TODO:\n",
        "            #is this created correctly? do i need to put the case study again??\n",
        "            messages=[\n",
        "            {'role': 'system', 'content': 'You answer questions about the case study. When the user asks for the results of a test, answer if the information is in the case study; otherwise say \"I do not have that information.\" Do not reveal diagnosis. Do not make claims that are not stated in the case study or make up statistics. Don’t justify your answers. Don’t give information not mentioned in the case study.'},\n",
        "            {'role': 'user', 'content': user_input + description + details + diagnosis},\n",
        "            #{'role': 'user', 'content': \"Do not reveal diagnosis. Do not make claims that are not stated in the case study or make up statistics. Don’t justify your answers. Don’t give information not mentioned in the case study.\"},\n",
        "            ]\n",
        "            response = openai.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                #max_tokens=100\n",
        "            )\n",
        "            print(response.choices[0].message.content)#.strip()\n",
        "\n",
        "        user_input = input(\"Your question or command: \")\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "\n",
        "# Example case studies (replace with actual case studies)\n",
        "#TODO:\n",
        "#preprocess case studies separately\n",
        "case_studies = [\n",
        "    {\n",
        "        \"description\": \"Patient presents with fever, cough, and shortness of breath.\",\n",
        "        \"details\": \"Test results show low oxygen levels. Patient has a history of smoking.\",\n",
        "        \"diagnosis\": \"Pneumonia\"\n",
        "    },\n",
        "    {\n",
        "        \"description\": \"Patient complains of abdominal pain, nausea, and vomiting.\",\n",
        "        \"details\": \"Test results show elevated liver enzymes. Patient recently traveled to a tropical region.\",\n",
        "        \"diagnosis\": \"Hepatitis\"\n",
        "    },\n",
        "    # Add more\n",
        "]\n",
        "\n",
        "# Pre-train the chatbot on the case studies\n",
        "trained_models = pretrain(case_studies)\n",
        "\n",
        "# multiple rounds\n",
        "while True:\n",
        "    # Select a pre-trained model\n",
        "    chatbot_model= select_model(trained_models)\n",
        "    # Get the description for the selected case study\n",
        "    #description = [case_study['description'] for case_study in case_studies if case_study['diagnosis'] == diagnosis][0]\n",
        "\n",
        "    # Play the round using the selected chatbot model\n",
        "    play_round(chatbot_model)\n",
        "\n",
        "    # Ask if the user wants to play another round\n",
        "    play_again = input(\"Do you want to start a new round? (yes/no): \")\n",
        "    if play_again.lower() != 'yes':\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CdJLUwCmYwi",
        "outputId": "f1921420-cbe5-4d7c-edea-82671021316d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New round started.\n",
            "Here's a brief description of the patient's case:\n",
            "Patient presents with fever, cough, and shortness of breath.\n",
            "Ask about specific details to diagnose the patient. Type 'reveal' to reveal the diagnosis. Type 'quit' to end the round.\n",
            "Your question or command: how long has the patient had these symptoms?\n",
            "I do not have that information.\n",
            "Your question or command: does the patient smoke?\n",
            "Yes, the patient has a history of smoking.\n",
            "Your question or command: reveal\n",
            "The diagnosis is: Pneumonia\n",
            "Your question or command: quit\n",
            "Do you want to start a new round? (yes/no): no\n"
          ]
        }
      ]
    }
  ]
}